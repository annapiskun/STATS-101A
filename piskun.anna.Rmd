---
title: "Final Exam, Stats 101A"
author: "Anna Piskun"
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[CO,CE]{Anna Piskun 505109399}
date: "3/12/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Part A

Question 1: B

Question 2A: C

Question 2B: A

Question 2C: B

Question 2D: D

Question 3: B

Question 4: B

Question 5: B

Question 6: A

Question 7A: Ho: all parameters (B1,B2,B3,B4,B5,B6) = 0

Question 7B: Ha: at least one parameter does not equal 0

Question 7C: Since the p-value is very small and less than 0.05, we reject the null hypothesis and conclude that at least one parameter (variable) does not equal 0. 

Question 9: D

Question 10: In the anova table, the partial F-statistics are interpreted "given that the variables that were entered before are controlled for." Thus, the output values for the same variable differ depending on the order in which the previous variables were organized in. Therefore, the output is the same for the num_drivers variable because they are the first variable in both tables, however, the losses output in the first table is that given num_drivers, perc_speeding, perc_alcohol, perc_not_distracted, and perc_no_previous are controlled for. Meanwhile, the output for losses in the second table is based solely on the fact that num_drivers is the only variable accounted for. 

### Part B: Applied

```{r}
atus <- read.csv("finalexam.csv")
```

Question 1: 

```{r}
t.test(sleep~holiday, data = atus)
```

a) t-statistic = -0.20665
b) p-value = 0.8414
c) Since the p-value is greater than 0.05, we fail to reject the null hypothesis and there is no difference in the mean number of minutes of sleep on holidays compared to other days. 

Question 2: 

```{r}
model1 <- lm(sleep~day, data = atus)
summary(model1)
```

a) 473.09 minutes 

Question 3:

```{r}
library(alr3)
model2 <- lm(sleep~age+eating+socializing+homework, data = atus)
plot(model2)
mmps(model2)
```

Looking at the residual plot, there is no clear pattern in the residuals indicating that the linearity condition is satisfied. Likewise, there is no fanshape in the residuals indicating that the constant variance condition is satisfied. However, the normal QQ plot has significant deviation at the tails, indicating that the normality of errors condition is not satisfied. The scale location plot shows no increasing or decreasing trend confirming that the constant variance condition is satisfied. There is no evidence of high leverage or influential points in the Residuals vs. Leverage plot. Additionally, looking at the marginal model plots, the loess lines and regression lines for certain predictors (socializing and eating for example) diverge. Therefore, according to the diagnostic plots and marginal model plots, the model is not valid. 

Question 4: 
```{r}
summary(model2)
```

The coefficient for age is -0.44724 and the p-value is 0.0553. Since the p-value is greater than 0.05, the variable age is not statistically significant and we fail to reject the null hypothesis. Therefore, there is no evidence that older people tend to sleep a different amount than others. 

Question 5: 

a)
```{r}
library(alr3)
invResPlot(model2)
```

The inverse response plot suggests raising the response variable sleep to the power of 4.963637. With the transformation, the RSS would decrease by over 1700 when compared to the RSS of the original model. Additionally, the transformed line deviates relatively significantly compared to the other lines. Therefore by transforming the response variable, more variation will be explained by our model and fit our data better. 

b) 
```{r}
summary(powerTransform(model2))
```

Looking at the boxcox method, since there is a small p-value (less than 0.05) for lambda = 0, we reject the null hypothesis that we should do a log transform. Looking at the p-value for lambda = 1 it is also less than 0.05, therefore we again reject the null hypothesis and a transformed model is best. Thus, the BoxCox method suggests raising the response variable sleep to the power of 0.82. 


Question 6: 

```{r}
atus.new <- transform(atus, sleep = sleep^0.82)
model3 <- lm(sleep~age+eating+socializing+homework, data = atus.new)
plot(model3)
summary(model3)
mmps(model3)
```

Looking at the residual plot, it is almost the exact same as the untransformed model - no patterns or fanshape, satisfying both the constant variance and linearity conditions. The normal QQ slightly reduced its deviation at the tails but not by much, the scale location plot is just as scattered as the untransformed model, and both still dont have any evidence of influential or high leverage points. Likewise, there is no improvement in the marginal model plots from the untransformed to the transformed model. Additionally, there is virtually no change in the adjusted r-squared from the untransformed to the transformed model. All in all, the model using the transformed sleep variable is almost the exact same as the untransformed model and does not significantly improve the model.

Question 7: 
```{r}
predictors <- dplyr::select(atus, sleep, age, eating, socializing, homework)
pairs(predictors)
vif(model2)
```

Collinearity is not an issue with these predictors, since there is no increasing or decreasing trends in the matrix plots. Additionally, for all four predictors they have a VIF < 5 (all are around 1). Thus none of these variables have too high VIF, and collinearity is not a problem. 

Question 8: 

```{r}
library(leaps)
levels(atus$day)
final.df <- dplyr::select(atus, sleep, age, socializing, homework, day, holiday)
model4 <- lm(sleep~., data = final.df)
summary(model4)

#best subsets regression
bestsubsets <- regsubsets(sleep~., data=final.df, nvmax=9)
plot(1:9, summary(bestsubsets)$bic)
summary(bestsubsets)$bic
summary(bestsubsets)
#2 variable model is best 

RSS <- summary(bestsubsets)$rss
n <- nrow(atus)
p <- 1:9
AIC <- n * log(RSS/n) + 2 * p
plot(1:9, AIC)
#7 variable model is best

```

a) Conducting best subsets regression, we find that according to BIC and AIC the best models are either the 2 variable or 7 variable model (BIC is smallest for 2 variables and AIC is smallest for 7 variables). Since this approach tests every single possible model combinations, we know that either the 2 variable or 7 variable model would be the best one. Furthermore, since the 2 variable model is simpler, it is preferred to the 7 variable one.

b) -41.85616

c) daySaturday and daySunday are the two variables that should be included in our best model. Therefore, we should just use $day$ because saturday and sunday are levels of the variable. 

Question 9: 

```{r}
final.model <- lm(sleep~day, data = atus)
summary(final.model)
predict(final.model, data.frame("day" = "Sunday"), interval = "prediction")
```

interval: [327.4566, 823.7434]



